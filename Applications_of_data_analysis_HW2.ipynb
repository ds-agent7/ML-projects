{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Майнор \"Прикладные задачи анализа данных\"\n",
    "## Домашнее задание 2 [10 баллов] до 23:59 22.03.2018. Предсказание цены акции по экономическим новостям\n",
    "\n",
    "\n",
    "В этом домашнем задании вы попытаетесь предсказать рост цены акции компании Газпром по новостям о компании. Домашнее задание состоит из трех частей:\n",
    "1. Предварительная обработка текстов и эксплоративный анализ\n",
    "2. Baseline алгоритм\n",
    "3. Творческая часть\n",
    "\n",
    "Все три части можно считать независимыми – вы можете сделать одну или две из них, однако мы настоятельно советуем выполнить все три. Все инструкции по выполнению домашнего задания – ниже. \n",
    "\n",
    "\n",
    "\n",
    "Входные данные:\n",
    "* Новости о компании \"Газпром\", начиная с 2010 года\n",
    "* Стоимость акций компании \"Газпром\" на ММВБ, начиная с 2010 года\n",
    "    * цена открытия (Open)\n",
    "    * цена закрытия (ClosingPrice)\n",
    "    * максимальная цена за день (DailyHigh)\n",
    "    * минимальная цена за день (DailyLow) \n",
    "    * объем бумаг (VolumePcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание делали: Захар Максименко, Анастасия Максимовская, Константин Ваниев (все - ИАД-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09.11.2017</td>\n",
       "      <td>Компания рассчитывает на решение по газовому с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08.11.2017</td>\n",
       "      <td>Как и предполагал “Ъ”, «Газпром», воспользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.11.2017</td>\n",
       "      <td>Новая редакция американских санкций ставит по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.10.2017</td>\n",
       "      <td>Как стало известно “Ъ”, известный на рынке ри...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.10.2017</td>\n",
       "      <td>НОВАТЭК, который через пять лет собирается за...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text\n",
       "0  09.11.2017  Компания рассчитывает на решение по газовому с...\n",
       "1  08.11.2017   Как и предполагал “Ъ”, «Газпром», воспользова...\n",
       "2  01.11.2017   Новая редакция американских санкций ставит по...\n",
       "3  30.10.2017   Как стало известно “Ъ”, известный на рынке ри...\n",
       "4  23.10.2017   НОВАТЭК, который через пять лет собирается за..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>DailyHigh</th>\n",
       "      <th>DailyLow</th>\n",
       "      <th>VolumePcs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08.12.2017</td>\n",
       "      <td>133.43</td>\n",
       "      <td>132.60</td>\n",
       "      <td>133.90</td>\n",
       "      <td>132.00</td>\n",
       "      <td>16037970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07.12.2017</td>\n",
       "      <td>133.70</td>\n",
       "      <td>133.02</td>\n",
       "      <td>133.87</td>\n",
       "      <td>132.81</td>\n",
       "      <td>18198430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06.12.2017</td>\n",
       "      <td>133.33</td>\n",
       "      <td>134.00</td>\n",
       "      <td>134.29</td>\n",
       "      <td>132.91</td>\n",
       "      <td>14641730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05.12.2017</td>\n",
       "      <td>133.48</td>\n",
       "      <td>133.65</td>\n",
       "      <td>133.99</td>\n",
       "      <td>132.78</td>\n",
       "      <td>12684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04.12.2017</td>\n",
       "      <td>133.01</td>\n",
       "      <td>133.77</td>\n",
       "      <td>134.00</td>\n",
       "      <td>131.93</td>\n",
       "      <td>17818980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open  ClosingPrice  DailyHigh  DailyLow  VolumePcs\n",
       "0  08.12.2017  133.43        132.60     133.90    132.00   16037970\n",
       "1  07.12.2017  133.70        133.02     133.87    132.81   18198430\n",
       "2  06.12.2017  133.33        134.00     134.29    132.91   14641730\n",
       "3  05.12.2017  133.48        133.65     133.99    132.78   12684800\n",
       "4  04.12.2017  133.01        133.77     134.00    131.93   17818980"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_all = pd.read_csv('gazprom_prices.csv', sep=';')\n",
    "pr_all['ClosingPrice'] = pr_all['ClosingPrice'].apply(lambda x: float(x.replace(',', '.')))\n",
    "pr_all['Open'] = pr_all['Open'].astype(str).apply(lambda x: float(x.replace(',', '.')))\n",
    "pr_all['DailyHigh'] = pr_all['DailyHigh'].astype(str).apply(lambda x: float(x.replace(',', '.')))\n",
    "pr_all['DailyLow'] = pr_all['DailyLow'].astype(str).apply(lambda x: float(x.replace(',', '.')))\n",
    "pr_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Вводная\n",
    "\n",
    "Проведите предобработку текстов: если считаете нужным, выполните токенизацию, приведение к нижнему регистру, лемматизацию и/или стемминг. Ответьте на следующие вопросы:\n",
    "* Есть ли корреляция между средней длинной текста за день и ценой закрытия?\n",
    "* Есть ли корреляция между количеством упоминаний Алексея Миллера  и ценой закрытия? Учтите разные варианты написания имени.\n",
    "* Упоминаний какого газопровода в статьях больше: \n",
    "    * \"северный поток\"\n",
    "    * \"турецкий поток\"?\n",
    "* Кого упоминают чаще:\n",
    "    * Алексея Миллера\n",
    "    * Владимира Путина?\n",
    "* Ско\n",
    "* О каких санкциях пишут в статьях?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from nltk.corpus import stopwords\n",
    "lemmed_texts = []\n",
    "df['text'] = df['text'].str.lower()\n",
    "reg = re.compile('[a-zа-я\\-]+')\n",
    "for i in range(len(df['text'])):\n",
    "    l1 = reg.findall( df['text'][i])\n",
    "    l3 = [morph.parse(token)[0].normal_form for token in l1 if not token in stopwords.words('russian')] \n",
    "    lemmed_texts.append(l3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ячейке выше мы токенизировали каждый документ в нашем маленьком корпусе. Также каждое слово приведено к строчным буквам, небуквенные символы (кроме дефиса) убраны, а слова приведены к начальной форме путем лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_tokenized = []\n",
    "for i in lemmed_texts:\n",
    "    j = (' ').join(i)\n",
    "    non_tokenized.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На всякий случай сделаем лемматизированную версию каждой новости без токенизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь дадим ответы на вопросы о том, кого чаще упоминают. \"Частота упоминания\", на наш взгляд, имеет два значения - количество уникальных упоминаний в документах корпуса (во скольких документах появится Х) и количество упоминаний вообще (сколько раз Х встречается во всех документах). Уникальные упоминания посчитаны ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def who_wins(first, second):\n",
    "    t1 = 0\n",
    "    t2 = 0\n",
    "    for i in non_tokenized:\n",
    "        if (i.find(first) != -1):\n",
    "            t1 += 1\n",
    "        if (i.find(second) != -1):\n",
    "            t2 += 1\n",
    "    print('Упоминания: ', first, \"-\", t1, ',', second, '-', t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Упоминания:  путин - 61 , миллер - 102\n"
     ]
    }
   ],
   "source": [
    "who_wins('путин', 'миллер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Упоминания:  владимир путин - 59 , алексей миллер - 102\n"
     ]
    }
   ],
   "source": [
    "who_wins('владимир путин', 'алексей миллер')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что добавление имени практически не поменяло количество уникальных упоминаний - почти всегда фамилия этих людей идет вместе с именем. Миллер появляется в большем числе документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Упоминания:  северный поток - 12 , турецкий поток - 34\n"
     ]
    }
   ],
   "source": [
    "who_wins('северный поток', 'турецкий поток')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Упоминания:  nord stream - 63 , south stream - 38\n"
     ]
    }
   ],
   "source": [
    "who_wins('nord stream', 'south stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если следовать \"букве\" задания, то можно сделать вывод, что северный поток появляется в документах чаще. Однако если сравнить количество уникальных упоминаний официальных международных наименований этих проектов, то становится очевидно, что северный поток обсуждается чаще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "                                    санкция ставить угроза зарубежный нефтяной \n",
      "Displaying 2 of 2 matches:\n",
      " участок интересоваться shell из-за санкция иностранец год шельф россия пока ид\n",
      " участок интересоваться shell из-за санкция иностранец год шельф россия пока ид\n",
      "Displaying 2 of 2 matches:\n",
      "компания рф наблюдаться число из-за санкция эксперт полагать сила опыт организа\n",
      "компания рф наблюдаться число из-за санкция эксперт полагать сила опыт организа\n",
      "Displaying 1 of 1 matches:\n",
      "изингодатель почувствовать западный санкция ъ рассказать гендиректор газпромбан\n",
      "Displaying 1 of 1 matches:\n",
      "рдить правомерность ввести совет ес санкция отношение роснефть брюссель запрети\n",
      "Displaying 2 of 2 matches:\n",
      "ение дело недовольный грозить банка санкция территория украина крупный российск\n",
      "ение дело недовольный грозить банка санкция территория украина\n",
      "Displaying 1 of 1 matches:\n",
      "жно-киринский месторождение попасть санкция мнение эксперт пока речь идти полит\n",
      "Displaying 1 of 1 matches:\n",
      "ение направление серьёзно осложнять санкция сша\n",
      "Displaying 2 of 2 matches:\n",
      "ка повышение ликвидность находиться санкция банка полтора-два год убыток превыш\n",
      "аланс собственный акция газпром фон санкция начинать предпринимать реальный дей\n",
      "Displaying 1 of 1 matches:\n",
      "дничество заморозить из-за западный санкция против банка владивосток спг отменн\n",
      "Displaying 1 of 1 matches:\n",
      "ткрывать спешить падение цена нефть санкция работа шельф сворачивать госкомпани\n",
      "Displaying 1 of 1 matches:\n",
      "страховка случай введение очередной санкция перевод расчёт экспорт российский н\n",
      "Displaying 1 of 1 matches:\n",
      "з газпром который сей пора западный санкция затрагивать впервые попасть удар сш\n",
      "Displaying 1 of 1 matches:\n",
      "вести западный страна экономический санкция против россия оао газпром начало го\n",
      "Displaying 1 of 1 matches:\n",
      "ему стремиться связывать крым из-за санкция репутационный риск вчера выясниться\n",
      "Displaying 2 of 2 matches:\n",
      "оект российский тэк момент введение санкция против рф получение технология инве\n",
      "оект российский тэк момент введение санкция против рф получение технология инве\n",
      "Displaying 1 of 1 matches:\n",
      "ь эмитент находиться антироссийский санкция конвертировать доллар занимать кото\n",
      "Displaying 1 of 1 matches:\n",
      "                                    санкция рнкб смп банк банк россия также пре\n",
      "Displaying 1 of 1 matches:\n",
      "о хотеть подвергать монополия риска санкция сторона ес сша переговоры еврокомис\n",
      "Displaying 1 of 1 matches:\n",
      "                                    санкция проблема привлечение финансирование\n",
      "Displaying 1 of 1 matches:\n",
      "облема найти финансирование условие санкция падение цена нефть\n",
      "Displaying 3 of 3 matches:\n",
      "ю телеканал россия- газпром ожидать санкция оказать значительный влияние финанс\n",
      "ндрей круглов возможный последствие санкция компания оценивать регулярно отмети\n",
      "ить доход проект напрямую затронуть санкция оказывать какой-либо существенный в\n",
      "Displaying 1 of 1 matches:\n",
      "                                    санкция несогласованный публикация заказчик\n",
      "Displaying 4 of 4 matches:\n",
      " нефть ожидать существенный влияние санкция финансовый показатель говориться от\n",
      "мя группа продолжать оценка влияние санкция считать оказать существенный влияни\n",
      " нефть ожидать существенный влияние санкция финансовый показатель говориться от\n",
      "мя группа продолжать оценка влияние санкция считать оказать существенный влияни\n",
      "Displaying 2 of 2 matches:\n",
      "мики правительство условие западный санкция готовый рассмотреть заявка другой н\n",
      "мики правительство условие западный санкция готовый рассмотреть заявка другой н\n",
      "Displaying 4 of 4 matches:\n",
      "                                    санкция западный страна против крупный росс\n",
      "барго вынудить заставить один волна санкция проигнорировать пошлый три волна са\n",
      "ия проигнорировать пошлый три волна санкция объявить де-факто финансово-экономи\n",
      "ть наш финансовый институт развитие санкция - объяснить министр\n",
      "Displaying 1 of 1 matches:\n",
      "это ключевой вопрос поскольку из-за санкция газпром закрытый западный рынок кап\n",
      "Displaying 4 of 4 matches:\n",
      "                                    санкция вслед роснефть лишиться exxonmobil \n",
      "о один проект сланцевый нефть shell санкция мочь затронуть другой сп salym petr\n",
      "ник продолжать терять партнёр из-за санкция вслед роснефть лишиться exxonmobil \n",
      "о один проект сланцевый нефть shell санкция мочь затронуть другой сп salym petr\n",
      "Displaying 4 of 4 matches:\n",
      "                                    санкция никак влиять деятельность существен\n",
      "азвитие компания из-за секторальный санкция сша ес ввести отношение россия газп\n",
      "азвитие компания из-за секторальный санкция сша ес ввести отношение россия серб\n",
      "миллиард рубль украина использовать санкция против газпром радикальный пересмот\n",
      "Displaying 1 of 1 matches:\n",
      "           газпромбанк гпб подпасть санкция сша ес полностью погасить трехлетни\n",
      "Displaying 2 of 2 matches:\n",
      "иться новый проблема из-за западный санкция технологический перспектива разрабо\n",
      "азаться вопрос именно каспий эффект санкция проявиться быстро остро\n",
      "Displaying 1 of 1 matches:\n",
      "                                    санкция список банк который получить возмож\n",
      "Displaying 2 of 2 matches:\n",
      "предоставление кредит дата введение санкция предоставление средство срок погаше\n",
      "исать контракт дата вступление сила санкция меняться момент - говориться разъяс\n",
      "Displaying 2 of 2 matches:\n",
      "                                    санкция сторона сша роснефть новатэк газпро\n",
      "т несмотря отсутствие прямой запрет санкция мочь привести трудность поставка об\n",
      "Displaying 2 of 2 matches:\n",
      "овладелец геннадий тимченко попасть санкция сша будущее выйти чёрный список ран\n",
      "ь ниже помочь вывод компания из-под санкция один канал вгтрк газпром-медиа наци\n",
      "Displaying 2 of 2 matches:\n",
      "                                    санкция сообщить президент газпром нефть ал\n",
      "редит из-за ввести отношение россия санкция сообщить президент газпром нефть ал\n",
      "Displaying 2 of 2 matches:\n",
      "ер мочь подпасть визовый финансовый санкция который европа планировать ввести о\n",
      "о касаться менеджер анализ ситуация санкция мочь распространиться деятельность \n",
      "Displaying 1 of 1 matches:\n",
      "нерго несколько год пытаться ввести санкция компания топ-менеджер неготовность \n",
      "Displaying 1 of 1 matches:\n",
      "транить наш опасение компания ждать санкция пока еврокомиссия получить хороший \n",
      "Displaying 2 of 2 matches:\n",
      "тать страна подпадать международный санкция куб россия который хотеть терять во\n",
      "тать страна подпадать международный санкция куб\n",
      "Displaying 1 of 1 matches:\n",
      "тво транзит однако никакой штрафной санкция монополия это предусмотреть ввод ко\n",
      "Displaying 1 of 1 matches:\n",
      "договориться год применять штрафной санкция нарушение контрактный обязательство\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "for i in range(len(lemmed_texts)):\n",
    "    if (non_tokenized[i].find('санкция') != -1):\n",
    "        Text(lemmed_texts[i]).concordance('санкция')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью вывода контекста для слова \"санкция\" в каждом документе, где встречается слово, можно понять, о каких санкциях идёт речь [хоть в лемматизированном тексте это не совсем обычно выглядит :) ]. Можно заметить, что речь идёт о санкциях США, санкциях, который вводит совет ЕС, санкциях Украины, западных санкциях. До периода российских политических проблем говорится о санкциях, наложенных на Кубу, а также о \"штрафных санкциях\", которые вообще совсем не политические. Много санкций хороших и разных!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prices = []\n",
    "for i in range(len(df['date'])):\n",
    "    if len(np.where(pr_all['Date'] == df['date'][i])[0]) > 0:\n",
    "        prices.append(pr_all['ClosingPrice'][np.where(pr_all['Date'] == df['date'][i])[0][0]])\n",
    "    else:\n",
    "        prices.append('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы строить какие-то корреляции, нужно четко сопоставить тексты и цены. Не для каждого текста есть цена, не для каждой цены есть текст. В ячейке выше мы создаем массив цены в день написания каждого доступного нам текста. Если цены нет, то мы ставим nan. Далее мы отбираем тексты, для которых есть цены, и из них формируем массивы значений. Первый с количеством упоминаний миллера (как мы выяснили в прошлом задании, между \"миллером\" и \"алексеем миллером\" разницы в количестве уникальных упоминаний практически нет, а вот по фамилии в рамках одного текста его могут назвать чаще в случае повторного упоминания). Второй с длинами текстов. Далее мы аналогичным образом отбираем цены, для которых есть тексты и считаем корреляцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_no_nan = []\n",
    "for i in np.where(np.array(prices) != 'nan')[0]:\n",
    "    texts_no_nan.append(lemmed_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "miller_texts = []\n",
    "for i in texts_no_nan:\n",
    "    miller_texts.append(i.count('миллер'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_texts = []\n",
    "for i in texts_no_nan:\n",
    "    len_texts.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prices_corr = np.array(prices)[np.where(np.array(prices) != 'nan')].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010635076467578013"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(miller_texts,prices_corr)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014823415198165359"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(len_texts,prices_corr)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляции нет. На наш взгляд, это связано с наличием множества других факторов, оказывающих влияние на цену, а также с большим временным разбросом новостей. То есть даже если бы упоминание Миллера имело бы связь с динамикой цены в конкретный день, существует множество факторов, из-за которых динамика может быть совершенно другой. Также связь с динамикой цены в конкретный день (если бы она была) не всегда означает высокую корреляцию - средний уровень цены акции в разные годы существенно отличается и по очевидным причинам зависит не от частоты упоминания Миллера в новостях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Классификационная\n",
    "Вам предстоит решить следующую задачу: по текстам новостей за день определить, вырастет или понизится цена закрытия.\n",
    "Для этого:\n",
    "* бинаризуйте признак \"цена закрытия\":  новый признак ClosingPrice_bin равен 1, если по сравнению со вчера цена не упала, и 0 – в обратном случаея;\n",
    "* составьте бучающее и тестовое множество: данные до начала 2016 года используются для обучения, данные с 2016 года и позже – для тестирования.\n",
    "\n",
    "Таким образом, в каждлый момент времени мы знаем: \n",
    "* ClosingPrice_bin – бинарый целевой признак\n",
    "* слова из статей, опубликованных в этот день – объясняющие признаки\n",
    "\n",
    "В этой части задания вам нужно сделать baseline алгоритм и попытаться его улучшить в следующей части. \n",
    "\n",
    "Используйте любой известный вам алгоритм классификации текстов для того, Используйте $tf-idf$ преобразование, сингулярное разложение, нормировку признакого пространства и любые другие техники обработки данных, которые вы считаете нужным. Используйте accuracy и F-measure для оценки качества классификации. Покажите, как  $tf-idf$ преобразование или сингулярное разложение или любая другая использованная вами техника влияет на качество классификации.\n",
    "Если у выбранного вами алгоритма есть гиперпараметры (например, $\\alpha$ в преобразовании Лапласа для метода наивного Байеса), покажите, как изменение гиперпараметра влияет на качество классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dynamics = np.zeros(len(df))\n",
    "for i in range(len(df)):\n",
    "    if len(pr_all[pr_all['Date'] == df['date'][i]]) != 0:\n",
    "        index = np.where(pr_all['Date'] == df['date'][i])[0][0]\n",
    "        if index == len(pr_all) - 1:\n",
    "            break\n",
    "        price = pr_all['ClosingPrice'][index]\n",
    "        prev_price = pr_all['ClosingPrice'][index + 1]\n",
    "        if type(price) == np.float64 and type(prev_price) == np.float64:\n",
    "            if price > prev_price:\n",
    "                dynamics[i] = 1\n",
    "        else:\n",
    "            dynamics[i] = 'nan'\n",
    "    else:\n",
    "        dynamics[i] = 'nan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь нужно создать бинарный признак. Описание алгоритма: Если для текста есть соответствующая дата в таблице с ценами и если эта дата - число, а не 'nan', то при условии, что цена за день с новостью больше предыдущей, то мы заносим в бинарный признак единицу. Если обратное - 0. Если нарушено одно из начальных условий - 'nan'. Далее мы убираем тексты и цены, для которых соответствия не нашлось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dynamics_match = []\n",
    "texts_match = []\n",
    "for i in range(len(dynamics)):\n",
    "    if not np.isnan(dynamics[i]):\n",
    "        dynamics_match.append(dynamics[i])\n",
    "        texts_match.append(non_tokenized[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем по-тупому. CountVectorizer просто сделает нам мешок слов, Tfidfvectorizer сделает мешок и взвесит слова в нём."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfstem = pd.DataFrame(data=dynamics_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(texts_match).toarray()\n",
    "df_cv = pd.DataFrame(data=x, columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что львиная доля работы - отбор признаков. В нашем случае - отбор слов, которые могут быть важными признаками для нашей модели. Критерием важности для модели сделаем сумму значений tf-idf для одного слова во всех документах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = []\n",
    "for i in range(df_cv.shape[1]):\n",
    "    counter.append(np.sum(df_cv.iloc[ :, [i]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея следующая: мы можем отобрать слова и проверить их важность для модели, зафитив целевую переменную с набором данных, состоящим из одного признака - значений tf-idf для какого-либо слова. Далее все слова, показавшие точность большую, чем некий уровень отсечения, определяются как важные признаки и вносятся в итоговый набор данных, на котором в итоге фитится основная модель. Мы получаем две переменные, от которых может зависеть результат - уровень суммы tf-idf для каждого слова, ниже которого слова мы брать не будем (оно может быть очень распространенным во всех документах и не иметь значения как различительный фактор, или напротив, оно может попасться 1-2 раза и его результат на accuracy может быть нерепрезентативным). Второй показатель - уровень отсечения accuracy, слова с уровнем ниже заданного не попадут в итоговую модель. Ниже представлены функция, реализующая этот алгоритм, а также способ отбора нужных значений переменных.\n",
    "\n",
    "Range подставляемых в обе переменные значений выявлен путём trial-and-error и сужением изначального range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "def feature_selector(tfidf_sum, threshold):\n",
    "    word_filter = []\n",
    "    y_train = dfstem[258:]\n",
    "    y_test = dfstem[:258]\n",
    "    for j in range(len(df_cv.columns)):\n",
    "        if counter[j] > tfidf_sum:\n",
    "            X_train = df_cv.filter([df_cv.columns[j]], axis = 1)[258:]\n",
    "            X_test = df_cv.filter([df_cv.columns[j]], axis = 1)[:258]\n",
    "            TradingGuru = GaussianNB()\n",
    "            TradingGuru.fit(X_train, y_train)\n",
    "            y_pred = TradingGuru.predict(X_test)\n",
    "            result = accuracy_score(y_test, y_pred)\n",
    "            if result > threshold:\n",
    "                word_filter.append(df_cv.columns[j])\n",
    "    X_train, y_train = df_cv.filter(word_filter, axis = 1)[258:], dfstem[258:]\n",
    "    X_test, y_test = df_cv.filter(word_filter, axis = 1)[:258], dfstem[:258]\n",
    "    TradingGuru = GaussianNB()\n",
    "    TradingGuru.fit(X_train, y_train)\n",
    "    y_pred = TradingGuru.predict(X_test)\n",
    "    result = accuracy_score(y_test, y_pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:432: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in subtract\n",
      "  (self.sigma_[i, :]), 1)\n"
     ]
    }
   ],
   "source": [
    "sums, thresholds, results = [], [], []\n",
    "for i in np.linspace(0.5,2,4):\n",
    "    for j in np.linspace(0.54, 0.57, 4):\n",
    "        sums.append(i)\n",
    "        thresholds.append(j)\n",
    "        results.append(feature_selector(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal tf-idf sum threshold: 1.0\n",
      "optimal accuracy threshold: 0.56\n",
      "accuracy score: 0.751937984496\n"
     ]
    }
   ],
   "source": [
    "print('optimal tf-idf sum threshold:', sums[np.where(results == np.max(results))[0][0]])\n",
    "print('optimal accuracy threshold:', thresholds[np.where(results == np.max(results))[0][0]])\n",
    "print('accuracy score:', np.max(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность составила около 76%. Чтобы понять, как бы было, если бы использовали простой CountVectorizer, сравним его результаты, полученные таким же образом, с полученными выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(token_pattern=r\"\\b\\w+\\b\")\n",
    "cv_matrix = count.fit_transform(texts_match).toarray()\n",
    "df_cv = pd.DataFrame(data=cv_matrix, columns=count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = []\n",
    "for i in range(df_cv.shape[1]):\n",
    "    counter.append(np.sum(df_cv.iloc[ :, [i]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_selector(tfidf_sum, threshold):\n",
    "    word_filter = []\n",
    "    y_train = dfstem[258:]\n",
    "    y_test = dfstem[:258]\n",
    "    for j in range(len(df_cv.columns)):\n",
    "        if counter[j] > tfidf_sum:\n",
    "            X_train = df_cv.filter([df_cv.columns[j]], axis = 1)[258:]\n",
    "            X_test = df_cv.filter([df_cv.columns[j]], axis = 1)[:258]\n",
    "            TradingGuru = GaussianNB()\n",
    "            TradingGuru.fit(X_train, y_train)\n",
    "            y_pred = TradingGuru.predict(X_test)\n",
    "            result = accuracy_score(y_test, y_pred)\n",
    "            if result > threshold:\n",
    "                word_filter.append(df_cv.columns[j])\n",
    "    X_train = df_cv.filter(word_filter, axis = 1)[258:]\n",
    "    X_test = df_cv.filter(word_filter, axis = 1)[:258]\n",
    "    TradingGuru = GaussianNB()\n",
    "    TradingGuru.fit(X_train, y_train)\n",
    "    y_pred = TradingGuru.predict(X_test)\n",
    "    result = accuracy_score(y_test, y_pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:432: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in subtract\n",
      "  (self.sigma_[i, :]), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.55\n",
      "0.0 0.56\n",
      "1.0 0.55\n",
      "1.0 0.56\n",
      "2.0 0.55\n",
      "2.0 0.56\n",
      "3.0 0.55\n",
      "3.0 0.56\n",
      "4.0 0.55\n",
      "4.0 0.56\n"
     ]
    }
   ],
   "source": [
    "sums, thresholds, results = [], [], []\n",
    "for i in np.linspace(0,4,5):\n",
    "    for j in np.linspace(0.55, 0.56, 2):\n",
    "        sums.append(i)\n",
    "        thresholds.append(j)\n",
    "        results.append(feature_selector(i, j))\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal countvect sum threshold: 0.0\n",
      "optimal accuracy threshold: 0.55\n",
      "accuracy score: 0.763565891473\n"
     ]
    }
   ],
   "source": [
    "print('optimal countvect sum threshold:', sums[np.where(results == np.max(results))[0][0]])\n",
    "print('optimal accuracy threshold:', thresholds[np.where(results == np.max(results))[0][0]])\n",
    "print('accuracy score:', np.max(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность совпадает, кроме того, CountVectorizer не потребовал никакой \"точки отсечения\" по количеству упоминаний слов во всех документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Творческая\n",
    "Придумайте и попытайтесь сделать еще что-нибудь, чтобы улучшить качество классификации. \n",
    "Направления развития:\n",
    "* Морфологический признаки: \n",
    "    * использовать в качестве признаков только существительные или только именованные сущности;\n",
    "* Модели скрытых тем:\n",
    "    * использовать в качестве признаков скрытые темы;\n",
    "    * использовать в качестве признаков динамические скрытые темы \n",
    "    пример тут: (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb)\n",
    "* Синтаксические признаки:\n",
    "    * использовать SOV-тройки в качестве признаков\n",
    "    * кластеризовать SOV-тройки по эмбеддингам глаголов (обученные word2vec модели можно скачать отсюда: (http://rusvectores.org/ru/models/) и использовать только центроиды кластеров в качестве признаков\n",
    "* что-нибудь еще     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем проделать первый предложенный вариант с использованием только существительных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_ru')\n",
    "from nltk import pos_tag\n",
    "nouns=[]\n",
    "for i in range(len(texts_match)):\n",
    "    l1 = reg.findall(texts_match[i])\n",
    "    l3 = [w for w , pos in pos_tag(l1, lang='rus') if (pos == 'S')]   # находим существительные\n",
    "    nouns.append(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_n = []\n",
    "for i in nouns:\n",
    "    j = (' ').join(i)\n",
    "    texts_n.append(j)        # делаем \"тексты\" только с существительными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_n = v.fit_transform(texts_n).toarray()\n",
    "df_cv_n = pd.DataFrame(data=x_n, columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter_n = []\n",
    "for i in range(df_cv_n.shape[1]):\n",
    "    counter_n.append(np.sum(df_cv_n.iloc[ :, [i]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_selector_n(tfidf_sum, threshold):\n",
    "    word_filter = []\n",
    "    y_train = dfstem[258:]\n",
    "    y_test = dfstem[:258]\n",
    "    for j in range(len(df_cv_n.columns)):\n",
    "        if counter_n[j] > tfidf_sum:\n",
    "            X_train = df_cv_n.filter([df_cv_n.columns[j]], axis = 1)[258:]\n",
    "            X_test = df_cv_n.filter([df_cv_n.columns[j]], axis = 1)[:258]\n",
    "            TradingGuru = GaussianNB()\n",
    "            TradingGuru.fit(X_train, y_train)\n",
    "            y_pred = TradingGuru.predict(X_test)\n",
    "            result = accuracy_score(y_test, y_pred)\n",
    "            if result > threshold:\n",
    "                word_filter.append(df_cv_n.columns[j])\n",
    "    X_train, y_train = df_cv_n.filter(word_filter, axis = 1)[258:], dfstem[258:]\n",
    "    X_test, y_test = df_cv_n.filter(word_filter, axis = 1)[:258], dfstem[:258]\n",
    "    TradingGuru = GaussianNB()\n",
    "    TradingGuru.fit(X_train, y_train)\n",
    "    y_pred = TradingGuru.predict(X_test)\n",
    "    result = accuracy_score(y_test, y_pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:432: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in subtract\n",
      "  (self.sigma_[i, :]), 1)\n"
     ]
    }
   ],
   "source": [
    "sums, thresholds, results = [], [], []\n",
    "for i in np.linspace(0.5,2,4):\n",
    "    for j in np.linspace(0.54, 0.57, 4):\n",
    "        sums.append(i)\n",
    "        thresholds.append(j)\n",
    "        results.append(feature_selector_n(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal tf-idf sum threshold: 1.5\n",
      "optimal accuracy threshold: 0.56\n",
      "accuracy score: 0.68992248062\n"
     ]
    }
   ],
   "source": [
    "print('optimal tf-idf sum threshold:', sums[np.where(results == np.max(results))[0][0]])\n",
    "print('optimal accuracy threshold:', thresholds[np.where(results == np.max(results))[0][0]])\n",
    "print('accuracy score:', np.max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_matrix_n = count.fit_transform(texts_n).toarray()\n",
    "df_cv_n = pd.DataFrame(data=cv_matrix_n, columns=count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter_n = []\n",
    "for i in range(df_cv_n.shape[1]):\n",
    "    counter_n.append(np.sum(df_cv_n.iloc[ :, [i]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_selector_n(tfidf_sum, threshold):\n",
    "    word_filter = []\n",
    "    y_train = dfstem[258:]\n",
    "    y_test = dfstem[:258]\n",
    "    for j in range(len(df_cv_n.columns)):\n",
    "        if counter_n[j] > tfidf_sum:\n",
    "            X_train = df_cv_n.filter([df_cv_n.columns[j]], axis = 1)[258:]\n",
    "            X_test = df_cv_n.filter([df_cv_n.columns[j]], axis = 1)[:258]\n",
    "            TradingGuru = GaussianNB()\n",
    "            TradingGuru.fit(X_train, y_train)\n",
    "            y_pred = TradingGuru.predict(X_test)\n",
    "            result = accuracy_score(y_test, y_pred)\n",
    "            if result > threshold:\n",
    "                word_filter.append(df_cv_n.columns[j])\n",
    "    X_train = df_cv_n.filter(word_filter, axis = 1)[258:]\n",
    "    X_test = df_cv_n.filter(word_filter, axis = 1)[:258]\n",
    "    TradingGuru = GaussianNB()\n",
    "    TradingGuru.fit(X_train, y_train)\n",
    "    y_pred = TradingGuru.predict(X_test)\n",
    "    result = accuracy_score(y_test, y_pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:432: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in subtract\n",
      "  (self.sigma_[i, :]), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.55\n",
      "0.0 0.56\n",
      "1.0 0.55\n",
      "1.0 0.56\n",
      "2.0 0.55\n",
      "2.0 0.56\n",
      "3.0 0.55\n",
      "3.0 0.56\n",
      "4.0 0.55\n",
      "4.0 0.56\n"
     ]
    }
   ],
   "source": [
    "sums, thresholds, results = [], [], []\n",
    "for i in np.linspace(0,4,5):\n",
    "    for j in np.linspace(0.55, 0.56, 2):\n",
    "        sums.append(i)\n",
    "        thresholds.append(j)\n",
    "        results.append(feature_selector_n(i, j))\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal countvect sum threshold: 0.0\n",
      "optimal accuracy threshold: 0.55\n",
      "accuracy score: 0.682170542636\n"
     ]
    }
   ],
   "source": [
    "print('optimal countvect sum threshold:', sums[np.where(results == np.max(results))[0][0]])\n",
    "print('optimal accuracy threshold:', thresholds[np.where(results == np.max(results))[0][0]])\n",
    "print('accuracy score:', np.max(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### К сожалению, в результате данного способа точность не повысилась, а понизилась. Возможно, слова других частей речи также играли роль в правильной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, можно в целом сказать, что предложенный во второй части алгоритм сам по себе является улучшением самого простого бейзлайна, т.к. мы предусмотрели такие параметры как accuracy threshold, tf-idf sum threshold, и отбирали признаки уже в самом бейзлайне, поэтому проведенный выше метод отбора существительных и не дал положительного результата"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как показало исследование, если взять самый незамысловатый бейзлайн, который просто после tf-idf преобразования будет проводить классификацию, у него будет намного ниже точность (0.47), а при отборе только существительных она ненамного, но повысится! Продемонстрируем это ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.472868217054\n"
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(texts_match).toarray()\n",
    "df_cv = pd.DataFrame(data=x, columns=v.get_feature_names())\n",
    "\n",
    "X_train, y_train = df_cv.iloc[258:], dfstem[258:]\n",
    "X_test, y_test = df_cv.iloc[:258], dfstem[:258]\n",
    "TradingGuru = GaussianNB()\n",
    "TradingGuru.fit(X_train, y_train)\n",
    "y_pred = TradingGuru.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nouns=[]\n",
    "for i in range(len(texts_match)):\n",
    "    l1 = reg.findall(texts_match[i])\n",
    "    l3 = [w for w , pos in pos_tag(l1, lang='rus') if (pos == 'S')]   # находим существительные\n",
    "    nouns.append(l3)\n",
    "\n",
    "texts_n = []\n",
    "for i in nouns:\n",
    "    j = (' ').join(i)\n",
    "    texts_n.append(j)        # делаем \"тексты\" только с существительными\n",
    "    \n",
    "x_n = v.fit_transform(texts_n).toarray()\n",
    "df_cv_n = pd.DataFrame(data=x_n, columns=v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.492248062016\n"
     ]
    }
   ],
   "source": [
    "X_train_n, y_train_n = df_cv_n.iloc[258:], dfstem[258:]\n",
    "X_test_n, y_test_n = df_cv_n.iloc[:258], dfstem[:258]\n",
    "TradingGuru = GaussianNB()\n",
    "TradingGuru.fit(X_train_n, y_train_n)\n",
    "y_pred_n = TradingGuru.predict(X_test_n)\n",
    "print(accuracy_score(y_test_n, y_pred_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сдача домашнего задания\n",
    "\n",
    "Дедлайн сдачи домашнего задания:  23:59 22.03.2018. Каждый день просрочки дедлайна штрафуется -1 баллом.\n",
    "\n",
    "Результаты домашнего задания должны быть оформлены в виде отчета в jupyter notebook.\n",
    "Нормальный отчёт должен включать в себя:\n",
    "* Краткую постановку задачи и формулировку задания\n",
    "* Описание минимума необходимой теории и/или описание используемых инструментов \n",
    "* Подробный пошаговый рассказ о проделанной работе\n",
    "* **Аккуратно** оформленные результаты\n",
    "* Подробные и внятные ответы на все заданные вопросы \n",
    "* Внятные выводы – не стоит относится к домашнему заданию как к последовательности сугубо технических шагов, а стоит относится скорее как к небольшому практическому исследованию, у которого есть своя цель и свое назначение.\n",
    "\n",
    "Задание выполняется в группе до трех человек. Не забудьте перечислить фамилии всех, кто работал над домашнем задании, в jupyter notebook.  \n",
    "\n",
    "В случае использования какого-либо строннего источника информации обязательно дайте на него ссылку (поскольку другие тоже могут на него наткнуться). Плагиат наказывается нулём баллов за задание и предвзятым отношением в будущем.\n",
    "\n",
    "\n",
    "При возникновении проблем с выполнением задания обращайтесь с вопросами к преподавателю по семинарским занятиям в вашей группе или у учебным ассистентам.\n",
    "\n",
    "Учебный ассистент по ДЗ 2: Таисия Глушкова (email: glushkovato@gmail.com, telegram: @glushkovato).\n",
    "\n",
    "\n",
    "Небрежное оформление отчета существенно отразится на итоговой оценке. Весь код из отчёта должен быть воспроизводимым, если для этого нужны какие-то дополнительные действия, установленные модули и т.п. — всё это должно быть прописано в отчете в явном виде.\n",
    "\n",
    "Сдача отчетов осуществляется через систему AnyTask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
